{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `<Rajat Singh>`<br/>\n",
    "** Roll Number: ** `<>`<br/>\n",
    "** Department: ** `<Industrial and Systems>`<br/>\n",
    "** Email: ** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "mu, sigma = 0.0, 1.0\n",
    "W1= np.random.normal(mu, sigma, size=(784, 512))\n",
    "W2 = np.random.normal(mu, sigma, size=(512, 256))\n",
    "W3 = np.random.normal(mu, sigma, size=(256, 10))\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):  #stable_softmax\n",
    "    ##############################write your code here #################################\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return np.divide(e_x, np.sum(e_x, axis = 1, keepdims=True))\n",
    "    ####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 16.6937 \n",
      " Epoch: 1, iteration: 937, Loss: 2.5603 \n",
      " Epoch: 2, iteration: 1874, Loss: 1.7269 \n",
      " Epoch: 3, iteration: 2811, Loss: 1.1513 \n",
      " Epoch: 4, iteration: 3748, Loss: 1.1513 \n",
      " Epoch: 5, iteration: 4685, Loss: 0.8635 \n",
      " Epoch: 6, iteration: 5622, Loss: 0.2900 \n",
      " Epoch: 7, iteration: 6559, Loss: 0.2878 \n",
      " Epoch: 8, iteration: 7496, Loss: 0.2878 \n",
      " Epoch: 9, iteration: 8433, Loss: 0.2878 \n",
      " Epoch: 10, iteration: 9370, Loss: 0.2878 \n",
      " Epoch: 11, iteration: 10307, Loss: 0.2878 \n",
      " Epoch: 12, iteration: 11244, Loss: 0.2878 \n",
      " Epoch: 13, iteration: 12181, Loss: 0.2878 \n",
      " Epoch: 14, iteration: 13118, Loss: 0.2878 \n",
      " Epoch: 15, iteration: 14055, Loss: 0.2878 \n",
      " Epoch: 16, iteration: 14992, Loss: 0.2878 \n",
      " Epoch: 17, iteration: 15929, Loss: 0.2878 \n",
      " Epoch: 18, iteration: 16866, Loss: 0.2878 \n",
      " Epoch: 19, iteration: 17803, Loss: 0.2878 \n",
      " Epoch: 20, iteration: 18740, Loss: 0.2878 \n",
      " Epoch: 21, iteration: 19677, Loss: 0.2878 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 20000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    y_batchinput = y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    \n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    # first hidden layer implementation\n",
    "    a1 = np.dot(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 = np.maximum(a1, 0) \n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2, 0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    \n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "     \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score = softmax_score - y_batchinput\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = np.dot(h2.T, grad_softmax_score) \n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = np.dot(grad_softmax_score, W3.T)\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = grad_h2*((a2>0).astype(int))\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = np.dot(h1.T, grad_a2) \n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.dot(grad_a2, W2.T)\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = grad_h1*((a1>0).astype(int))\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = np.dot(x_batchinput.T, grad_a1) \n",
    "    ###############################################################################################\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 91.16 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    # first hidden layer implementation\n",
    "    a1 = np.dot(x_batchinput, W1)\n",
    "    # implement Relu layer\n",
    "    h1 =  np.maximum(a1, 0) \n",
    "    #  implement 2nd hidden layer\n",
    "    a2 = np.dot(h1, W2)\n",
    "    # implement Relu activation \n",
    "    h2 = np.maximum(a2, 0)\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2, W3)\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
